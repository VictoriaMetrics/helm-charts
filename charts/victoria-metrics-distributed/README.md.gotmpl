# Victoria Metrics Helm Chart for Running VMCluster on Multiple Availability Zones

{{ template "chart.typeBadge" . }} {{ template "chart.versionBadge" . }}
[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/victoriametrics)](https://artifacthub.io/packages/helm/victoriametrics/victoria-metrics-distributed)
[![Slack](https://img.shields.io/badge/join%20slack-%23victoriametrics-brightgreen.svg)](https://slack.victoriametrics.com/)

{{ template "chart.description" . }}

## Prerequisites

* Install the follow packages: ``git``, ``kubectl``, ``helm``, ``helm-docs``. See this [tutorial](../../REQUIREMENTS.md).

* PV support on underlying infrastructure.

* Multiple availability zones.

## Chart Details

This chart is for setting up multiple VictoriaMetrics cluster instances on multiple [availability zones](https://kubernetes.io/docs/setup/best-practices/multiple-zones/). It includes both per availability zone components and global components.
The default topology is like
![victoriametrics-distributed-topology](./victoriametrics-distributed-topology.jpg)

Per availability zone components:
1. vmagent: remotes write to all availability zones to ensure data completeness.
2. vmauth-ingest: performs l`east_loaded` policy for spreading requests to vminsert instances located in the same availability zone.
3. vmcluster: supports different setup per availability zone.
4. vmauth-query: performs `least_loaded` policy for spreading requests to vmselect instances located in the same availability zone.
5. vmauth-query-cross-az: performs `first_available` policy on vmselect instances of all the availability zones, prefers "local" vmselect to reduce cross zone network traffic.

Global components:
1. vmagent: an extra vmagent to scrape targets and remote write to `vmauth-global-ingest`.
2. vmauth-global-ingest: global write entrance.
3. vmauth-global-query: global query entrance.
4. grafana

Note:
As the diagram showed above, this chart doesn't include components like vmalert, alertmanager, etc by default. If needed, you can create those resources using victoria-metrics-k8s-stack chart.

### When&Why use `victoria-metrics-distributed` chart?

One of the best practice for running production kubernetes cluster is running with [multiple availability zones](https://kubernetes.io/docs/setup/best-practices/multiple-zones/). And to avoid zone outage, we also want to spread our application pods on multiple availability zones.
For VictoriaMetrics, there are two ways:
1. set up one vmcluster but spread the components on different zones by using `topologySpreadConstraints`.
For example, set up vmcluster with two vminsert&vmstorage&vmselect instances and `replicationFactor=2`, and set `topologySpreadConstraints` for them like:
```
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
```
Then each zone has a complete set of vminsert&vmstorage&vmselect, vmcluster can still serve write and query requests when one zone failure.
But VictoriaMetrics components themselves don't support zone-awareness, means in the above example, if you create more than two vmstorage nodes and `replicationFactor=2`, vmstorage can't ensure that duplicated data be written to vmstorage instances on different availability zone, thus data could be incomplete when one availability zone is down.
2. use `victoria-metrics-distributed` chart here, set up separate vmcluster on each availability zone, write same data to all of them, and query from any of them.

### How to use multitenancy?

By default, all the data that write to `vmauth-global-ingest` will be stored with tenant 0. If you want to write to different tenants, creating extra VMUser for `vmauth-global-ingest`.
For example, ingest data under tenant `1088` with follow steps:
1. create tenant VMUser for vmauth `vmauth-global-ingest` to use:
```
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMUser
metadata:
  name: tenant-1088-rw
  labels:
    tenant-test: "true"
spec:
  targetRefs:
  - static:
      ## list all the zone vmagent here
      url: "http://vmagent-vmagent-zone-a:8429"
      url: "http://vmagent-vmagent-zone-b:8429"
    paths:
    - "/api/v1/write"
    - "/prometheus/api/v1/write"
    - "/write"
    - "/api/v1/import"
    - "/api/v1/import/.+"
    target_path_suffix: /insert/1088/
  username: tenant-1088
  password: dangerous
```

add vmuser selector in vmauth `vmauth-global-ingest`
```
spec:
  userSelector:
    matchLabels:
      tenant-test: "true"
```

2. send data to `vmauth-global-ingest` with tenant user's token
Example command for writing the data using vmagent for above tenant 1088:
```
/path/to/vmagent -remoteWrite.url=http://vmauth-vmauth-global-ingest:8427/prometheus/api/v1/write -remoteWrite.basicAuth.username=tenant-1088 -remoteWrite.basicAuth.password=dangerous
```

### How to roll out components?

All the components under each availability zone can be configured separately, it's recommend to upgrade one zone at a time, stop ingesting and querying until the upgrade is completed.
```
availabilityZones:
  # stop ingest and query from zone-a until upgrade is completed
  - name: zone-a
    # allow data ingestion to this zone
    allowIngest: false
    # allow data query from this zone through global query endpoint
    allowQuery: false
```


## How to install

Access a Kubernetes cluster.

Add a chart helm repository with follow commands:

```console
helm repo add vm https://victoriametrics.github.io/helm-charts/

helm repo update
```

List versions of ``vm/victoria-metrics-distributed``` chart available to installation:

```console
helm search repo vm/victoria-metrics-distributed` -l
```

Export default values of ``victoria-metrics-distributed``` chart to file ``values.yaml``:

```console
helm show values vm/victoria-metrics-distributed` > values.yaml
```

Change the values according to the need of the environment in ``values.yaml`` file.

Test the installation with command:

```console
helm install vmcluster vm/victoria-metrics-distributed` -f values.yaml -n NAMESPACE --debug --dry-run
```

Install chart with command:

```console
helm install vmcluster vm/victoria-metrics-distributed` -f values.yaml -n NAMESPACE
```

Get the pods lists by running this commands:

```console
kubectl get pods -A | grep 'vminsert\|vmselect\|vmstorage'
```

Get the application by running this command:

```console
helm list -f vmcluster -n NAMESPACE
```

See the history of versions of ``vmcluster`` application with command.

```console
helm history vmcluster -n NAMESPACE
```

## How to uninstall

Remove application with command.

```console
helm uninstall vmcluster -n NAMESPACE
```

## Documentation of Helm Chart

Install ``helm-docs`` following the instructions on this [tutorial](../../REQUIREMENTS.md).

Generate docs with ``helm-docs`` command.

```bash
cd charts/victoria-metrics-distributed`

helm-docs
```

The markdown generation is entirely go template driven. The tool parses metadata from charts and generates a number of sub-templates that can be referenced in a template file (by default ``README.md.gotmpl``). If no template file is provided, the tool has a default internal template that will generate a reasonably formatted README.

## Parameters

The following tables lists the configurable parameters of the chart and their default values.

Change the values according to the need of the environment in ``victoria-metrics-distributed`/values.yaml`` file.

{{ template "chart.valuesTable" . }}
