---
weight: 5
title: VictoriaMetrics Agent
menu:
  docs:
    parent: helm
    weight: 5
    identifier: helm-victoriametrics-agent
url: /helm/victoriametrics-agent
aliases:
  - /helm/victoriametrics-agent/index.html
  - /helm/victoria-metrics-agent/index.html
tags:
  - metrics
  - kubernetes
---
{{ template "chart.badges" . }}

{{ template "chart.description" . }}

## Prerequisites

* Install the follow packages: ``git``, ``kubectl``, ``helm``, ``helm-docs``. See this [tutorial](https://docs.victoriametrics.com/helm/requirements/).

{{ include "chart.installSection" . }}

## Upgrade guide

### Upgrade to 0.13.0

- replace `remoteWriteUrls` to `remoteWrite`:

Given below config

```yaml
remoteWriteUrls:
- http://address1/api/v1/write
- http://address2/api/v1/write
```

should be changed to

```yaml
remoteWrite:
- url: http://address1/api/v1/write
- url: http://address2/api/v1/write
```

{{ include "chart.uninstallSection" . }}

{{ include "chart.helmDocs" . }}

## Examples

### Daemonset mode

 It's possible to configure vmagent to use [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) instead of Deployment and StatefulSet.

Key features:
* reduce network traffic for metric scrapping.
* spread load for metrics collection.
* provide resilience for single pod failure.

 In this scenario, vmagent pods will be launched one per each Kubernetes Node. It is required to manually adjust the scrape configuration with `spec.nodeName` pod [field selector](https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/#list-of-supported-fields) for Kubernetes API requests.
vmagent `kubernetes_sd_configs` section must have either `role: pod` or `role: node`. Other roles could lead to excessive CPU and memory usage, and potentially may overload Kubernetes API server.
An example of configuration:
```yaml
mode: daemonSet

env:
  - name: KUBE_NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName

remoteWrite:
   # replace with your remote write url
  - url: http://vmsingle-vms-victoria-metrics-k8s-stack:8428/api/v1/write

config:
  global:
    scrape_interval: 10s

  scrape_configs:
    - job_name: "kubernetes-nodes"
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        # filter node for local one
        - action: keep
          source_labels: [__meta_kubernetes_node_name]
          regex: "^%{KUBE_NODE_NAME}$"

        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/$1/proxy/metrics

    - job_name: "kubernetes-nodes-cadvisor"
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        # filter node for local one
        - action: keep
          source_labels: [__meta_kubernetes_node_name]
          regex: "^%{KUBE_NODE_NAME}$"
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
      honor_timestamps: false

    - job_name: "kubernetes-pods"
      kubernetes_sd_configs:
        - role: pod
          selectors:
            # use server side selector for pods
            - role: pod
              field: spec.nodeName=%{KUBE_NODE_NAME}
      relabel_configs:
        - action: drop
          source_labels: [__meta_kubernetes_pod_container_init]
          regex: true
        - action: keep_if_equal
          source_labels:
            [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_container_name]
          target_label: container
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: node```

 daemonSet mode has the following restrictions and limitations:
* sharding not supported.
* podDisruptionPudget not supported.
* horizontalPodAutoScraler not supported.
* Volume for the persistent-queue could be mounted with `extraVolumes` and `extraVolumeMounts` and must have hostPath source path.
* vmagent restarts will lead to the small metric collection gaps. Only a single pod from DaemonSet deployed per node.

## Parameters

The following tables lists the configurable parameters of the chart and their default values.

Change the values according to the need of the environment in ``victoria-metrics-agent/values.yaml`` file.

{{ template "chart.valuesTableHtml" . }}
